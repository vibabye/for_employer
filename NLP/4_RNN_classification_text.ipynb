{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ЗАДАНИЕ 1**\n",
        "\n",
        "Для датасета с 18000 новостей, сгруппированных по 20 темам, выберете 4 темы и используйте модель RNN для классификации новостей (например, \"спорт\", \"политика\", \"технологии\" и т.д.). Объясните, как работает RNN модель в задачах классификации текстов, какие предобработки данных вы применяете и какие результаты вы получили в процессе обучения и тестирования модели"
      ],
      "metadata": {
        "id": "dVhfhhF0OOpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN (Recurrent Neural Network)** - это тип нейронной сети, способной обрабатывать последовательные данные, такие как текст.\n",
        "\n",
        "В задачах классификации текстов RNN может быть использована для анализа и учета последовательной структуры текста. Вот как работает RNN модель в задачах классификации текстов:\n",
        "\n",
        "1. Последовательный вход: Текст представляется в виде последовательности слов или токенов. Каждое слово кодируется в числовой формат, обычно с использованием векторных представлений, таких как word embeddings (например, Word2Vec или GloVe).\n",
        "2. Рекуррентные слои: RNN содержит один или несколько рекуррентных слоев. Эти слои позволяют модели учитывать контекст и зависимости между словами в тексте. Каждый слой принимает на вход текущее слово и состояние, полученное из предыдущего слова, и выдает новое состояние.\n",
        "3. Предсказание класса: Последний состояний рекуррентного слоя или выход последнего временного шага передается через полносвязанный (Dense) слой с функцией активации, которая соответствует числу классов в задаче классификации. Это выходной слой, который предсказывает класс, к которому принадлежит текст.\n",
        "4. Обучение: Модель обучается с использованием обучающих данных и функции потерь, такой как кросс-энтропия. Обратное распространение ошибки используется для настройки весов модели таким образом, чтобы минимизировать потери.\n",
        "\n",
        "Преимущества RNN в задачах классификации текстов:\n",
        "\n",
        " • Способность учитывать контекст и зависимости между словами, что позволяет лучше анализировать смысл текста.\n",
        " • Подходит для текстов разной длины, так как RNN обрабатывает последовательности переменной длины.\n",
        "\n",
        "Однако у RNN есть и недостатки, такие как проблема исчезающего градиента. Поэтому в практике часто используются усовершенствованные архитектуры, такие как LSTM (Long Short-Term Memory) и GRU (Gated Recurrent Unit), чтобы бороться с этими ограничениями и улучшить производительность в задачах классификации текстов."
      ],
      "metadata": {
        "id": "uk2BofiDI7t2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A94DoZcCHMXs",
        "outputId": "bda5408e-a5d7-4afa-dc8b-3960f46b432c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3387,)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism', 'talk.religion.misc',\n",
        "              'comp.graphics', 'sci.space']\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset='all',\n",
        "                                      categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups.filenames.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(newsgroups.data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puyscP4YIRie",
        "outputId": "a25e322a-eeca-4df9-81e5-385040b0cba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My point is that you set up your views as the only way to believe.  Saying \n",
            "that all eveil in this world is caused by atheism is ridiculous and \n",
            "counterproductive to dialogue in this newsgroups.  I see in your posts a \n",
            "spirit of condemnation of the atheists in this newsgroup bacause they don'\n",
            "t believe exactly as you do.  If you're here to try to convert the atheists \n",
            "here, you're failing miserably.  Who wants to be in position of constantly \n",
            "defending themselves agaist insulting attacks, like you seem to like to do?!\n",
            "I'm sorry you're so blind that you didn't get the messgae in the quote, \n",
            "everyone else has seemed to.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Выбор только необходимых тем\n",
        "filtered_data = []\n",
        "filtered_labels = []\n",
        "for i, topic in enumerate(categories):\n",
        "    mask = np.where(newsgroups.target == i)\n",
        "    filtered_data.extend(np.array(newsgroups.data)[mask])\n",
        "    filtered_labels.extend([i] * len(mask[0]))\n",
        "\n",
        "# Предобработка данных\n",
        "max_words = 10000  # Максимальное количество слов в словаре\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(filtered_data)\n",
        "X = tokenizer.texts_to_sequences(filtered_data)\n",
        "X = pad_sequences(X)\n",
        "\n",
        "# Преобразование меток в one-hot кодировку\n",
        "y = np.array(filtered_labels)\n",
        "num_classes = len(categories)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes)"
      ],
      "metadata": {
        "id": "S-dpnYS9Mx_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=X.shape[1]))\n",
        "model.add(SimpleRNN(128))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wrN6BHePNOs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N-tLm46NUND",
        "outputId": "b78b36f3-698b-4056-b21d-b3e9927fba67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "43/43 [==============================] - 651s 15s/step - loss: 1.3805 - accuracy: 0.2997 - val_loss: 1.3704 - val_accuracy: 0.3127\n",
            "Epoch 2/5\n",
            "43/43 [==============================] - 648s 15s/step - loss: 1.3383 - accuracy: 0.3606 - val_loss: 1.3487 - val_accuracy: 0.3142\n",
            "Epoch 3/5\n",
            "43/43 [==============================] - 640s 15s/step - loss: 1.3346 - accuracy: 0.3614 - val_loss: 1.3545 - val_accuracy: 0.2920\n",
            "Epoch 4/5\n",
            "43/43 [==============================] - 645s 15s/step - loss: 1.3149 - accuracy: 0.3957 - val_loss: 1.3482 - val_accuracy: 0.3776\n",
            "Epoch 5/5\n",
            "43/43 [==============================] - 645s 15s/step - loss: 1.2830 - accuracy: 0.4710 - val_loss: 1.3176 - val_accuracy: 0.3761\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a763eba9900>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Точность проверки на валидационных данных показывает, насколько хорошо модель способна обобщать свои предсказания на неизвестные данные. Так как оценка эффективности довольно таки низкая, то мы можем сказать, что модель плохо обощает свои предскзания на неизвестные данные.\n",
        "\n",
        "На 3 эпохе в связи с понижением валидационной оценки модели, происходит недообучение, так же об этом говорит ошибка, которая выросла. На 4 эпохе все нормализуется, ошибка падает, оценка растет.\n",
        "\n",
        "Общая accuracy должна быть выше валидационного, что у нас и имеется, тем не менее она тоже имеет низкое значение, это значит, что наша модель плохо обучена (недообучена)"
      ],
      "metadata": {
        "id": "RwQW5YwzXFWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooVUDvaANWX7",
        "outputId": "8a8d2a04-fb72-4cb4-93ed-4663be8e0395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 16s 747ms/step - loss: 1.3176 - accuracy: 0.3761\n",
            "Loss: 1.3175781965255737, Accuracy: 0.3761062026023865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ЗАДАНИЕ 2**\n",
        "\n",
        "используя алгоритмы, такие как Word2Vec или GloVe, необходимо построить эмбеддинги для вашего датасета (по заданию 1). Вывести примеры «близких» слов. Сделайте выводы"
      ],
      "metadata": {
        "id": "dFuaBIbAOZOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83MfzJeuOcml",
        "outputId": "ae51acfd-de44-4a3e-aeea-487753821361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "tokenized_data = [text.split() for text in filtered_data]\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, sg=0)"
      ],
      "metadata": {
        "id": "7Er_xVuJRM_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sports_vector = word2vec_model.wv['sports']\n",
        "\n",
        "similar_words = word2vec_model.wv.most_similar('sports', topn=5)\n",
        "\n",
        "for word, score in similar_words:\n",
        "    print(f\"Слово: {word}, Близость: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CdZm7wdRP4W",
        "outputId": "f78127a6-a621-47ad-80af-ca954707010f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Слово: Revelation, Близость: 0.9721689820289612\n",
            "Слово: Arianespace, Близость: 0.9684611558914185\n",
            "Слово: VIKING, Близость: 0.9683914184570312\n",
            "Слово: Cassini, Близость: 0.967854380607605\n",
            "Слово: orbiter, Близость: 0.9665995836257935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По резульататам предыдущего кода мы можем увидеть, что качетсво наших эмбеддингов очень хорошее, так как близость высокая."
      ],
      "metadata": {
        "id": "9gjw2u62aZ3h"
      }
    }
  ]
}